---
layout: post
title: "LAB REPORT Nine"
tags: [lab report, fieldbook]
author: AVERY BLANKENSHIP
---
-------------------

### Lab Report Number Nine
-

_**Prompt:**_

Open response, drawing on the lab and readings.
-


_**Response:**_

This week, we focused on some basic data analysis techniques such as sentiment analysis and topic modeling. I've worked with both before in Python, but I had never done them in R, so I found this lab to be very useful for learning how simple R makes these processes since R is a language that is designed for this type of statistical analysis. As we worked through the lab, I found myself thinking back to the "Against Cleaning" reading. At each step of the data analysis cycle, there were some questions that we had to ask ourselves regarding how we treat the data and what data we consider in our experiment. In the newspaper CSV that we were working with, there were some dates that were marked "9999" which meant that the date was unknown and we had to make a decision regarding how to treat these unknowns.

Personally, I think that it is impossible for data to be completely unbiased because the act of collecting data implies some heuristic that the researcher is using to make decisions regarding which data is included and which data is excluded. However, I think that it should be the goal of any researcher to attempt to make their data and their heuristic as unbiased as possible. As we worked on the lab, I found myself wondering whether or not changing the dates changes the data in some way that presents a level of interference with the results. If there is an obvious typo in a data set, should the researcher fix it, or should they exclude that data from the  analysis? Should there be a separate column added to the data which includes the altered data next to the original? A lot of these thoughts that I had made me think back to our discussion of messy OCR and how even OCR that is "messy" can teach a researcher a lot. I think  that when working with data, there are a lot of tough questions that a researcher has to ask themselves. Is it better to change the data to make it more legible to a viewer, or is it better to leave the data alone so as not to introduce a level of bias?

I also found myself thinking about a lot of the results of our data. I know that topic modeling includes some level of randomness to it (in that every time you run a topic model, the results change) and I found myself thinking about how often I read pieces where topic modeling is treated like some static analysis that you can run on some data and get the same result every time. I wondered whether unfaithful portrayal of an  analysis in order to make it easier to understand, is in itself, also a form of "cleaning." 
